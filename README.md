## SÃ©ries Temporais
#### Objetivos:
- Analisar a origem da sÃ©rie
- PrevisÃµes futuras
- DescriÃ§Ã£o do comportamento da sÃ©rie 
- Analisar perodicidade ou tendÃªncia 

#### Tipos:
- Univariada = apenas uma variÃ¡vel se altera ao longo do tempo
- Multivariada = mais de uma variÃ¡vel se altera ao longo do tempo

#### Conceitos:
SÃ©rie Temporal -> Ã© um conjunto de observaÃ§Ãµes ordenadas no tempo ou um corte particular de um processo estocÃ¡stico desconhecido

#### Matematicamente: Y = Tdt + Szt + et 
- TendÃªncia (Tdt): MudanÃ§as graduais em longo prazo (crescimento populacional).
- Sazonalidade (Szt): oscilaÃ§Ãµes de subida e de queda que sempre ocorrem em um determinado perÃ­odo (maior valor da conta de energia elÃ©trica no inverno).
- ResÃ­duos (et): apresenta movimentos ascendentes e descendentes da sÃ©rie apÃ³s a retirada do efeito de tendÃªncia ou sazonal (sequÃªncia de variÃ¡veis aleatÃ³rias).

![Figura-4-Decomposicao-da-serie-temporal-em-componentes-de-sazonalidade-de-tendencia-e](https://github.com/HenrySchall/Time-Series/assets/96027335/46bf2b49-dcb1-4153-9d66-2e57b4dc57ad)

Processo EstocÃ¡stico -> Ã© uma coleÃ§Ã£o de variÃ¡veis aleatÃ³rias definidas num mesmo espaÃ§o de probabilidades (processo gerador de uma sÃ©rie de variÃ¡veis). A descriÃ§Ã£o de um 
processo estocÃ¡stico Ã© feita atravÃ©s de uma distribuiÃ§Ã£o de probabilidade conjunta (o que Ã© muito complexo de se fazer), entÃ£o geralmente descrevemos ele por meio das funÃ§Ãµes:
- $ğœ‡(ğ‘¡)=ğ¸{ğ‘(ğ‘¡)}$ -> MÃ©dia 
- $ğœ^2(ğ‘¡)=ğ‘‰ğ‘ğ‘Ÿ{ğ‘(ğ‘¡)}$ -> VariÃ¢ncia 
- $ğ›¾(ğ‘¡1,ğ‘¡2)=ğ¶ğ‘œğ‘£{ğ‘(ğ‘¡1),ğ‘(ğ‘¡2)}$ -> AutocovariÃ¢ncia

![Captura de tela 2024-07-04 180109](https://github.com/HenrySchall/Time-Series/assets/96027335/7ffc0399-4f35-4e82-ac69-8950c083c8f4)

Estacionaridade -> Ã© quando uma sÃ©rie temporal apresenta todas suas caracterÃ­sticas estatÃ­sticas constante ao longo do tempo
- Estacionaridade Fraca = Ã© quando as propriedades estatiaticas, sÃ£o constantes no tempo, E(x)=U, Var(x) = ğœ^2, COV(X,X-n) = k (corariÃ¢ncia entre observaÃ§Ãµes em diferentes pontos no tempo depende do tempo especÃ­fico em que elas ocorreram). Na literatura, geralmente estacionalidade significa estacionalidade fraca.

- Estacionaridade Forte = tambÃ©m chamada de estrita, Ã© quando a funÃ§Ã£o de probabilidade conjunta Ã© invariante no tempo, ou seja, as distribuiÃ§Ãµes individuais sÃ£o iguais para todos "ts". Com isso a covariÃ¢ncia depende apenas da distÃ¢ncia entre as observaÃ§Ãµes e nÃ£o do tempo especifico que ocorreram. 

![Imagem-2](https://github.com/HenrySchall/Time-Series/assets/96027335/6c237676-00e5-407f-bcc7-cddf6c1c4a34)

Passeio AleatÃ³rio (Random Walk) -> Ã© a soma de pequenas flutuaÃ§Ãµes estocÃ¡sticas (tendÃªncia estocÃ¡stica)
Matematicamente: $ğ‘ğ‘¡ = ğ‘(ğ‘¡âˆ’1)+ et$

AutocorrelaÃ§Ã£o -> Ã© a correlaÃ§Ã£o de determinados perÃ­odos anteriores com o perÃ­odo atual, ou seja, o grau de dependÃªncia serial. Cada perÃ­odo desse tipo de correlaÃ§Ã£o Ã© denominado lag (defasagem) e sua representaÃ§Ã£o Ã© feita pela FunÃ§Ã£o de AutocorrelaÃ§Ã£o (FAC) e a FunÃ§Ã£o de AutocorrelaÃ§Ã£o Parcial (FACP), ambas comparam o valor presente com os valores passados da sÃ©rie, a diferenÃ§a entre eles Ã© que a FAC analisa tanto a correlaÃ§Ã£o direta como a indireta, jÃ¡ a FACP apenas correlaÃ§Ã£o direta. EntÃ£o podemos dizer, que a FAC vÃª a correlaÃ§Ã£o direta do mÃªs de janeiro em marÃ§o e tambÃ©m a correlaÃ§Ã£o indireta que o mÃªs de janeiro teve em fevereiro que tambÃ©m teve em marÃ§o, enquanto que a FACP apenas a correlaÃ§Ã£o de janeiro em marÃ§o. Essa anÃ¡lise Ã© feita, porque Ã© o pressuposto essencial para se criar previsÃµes eficientes de uma sÃ©rie.

RuÃ­do Branco (White Noise) -> Ã© quando o erro de uma sÃ©rie temporal, segue uma distribuiÃ§Ã£o normal, ou seja, um processo puramente aleatÃ³rio. 
- $E(Xt) = 0$ 
- $Var(Xt) = ğœ^2$

TransformaÃ§Ã£o e SuavizaÃ§Ã£o -> SÃ£o tÃ©cnicas que buscam deixar a sÃ©rie o mais prÃ³ximo possÃ­vel de uma distribuiÃ§Ã£o normal. Transformando o valor das varÃ¡veis ou suavizando a tendÃªncia e/ou sazonaliade da sÃ©rie. Dentre todas as tÃ©cnicas existentes podemos citar:
1) TranformaÃ§Ã£o Log 
2) TranformaÃ§Ã£o Expoencial
3) TranformaÃ§Ã£o Box-Cox
4) SuavizaÃ§Ã£o MÃ©dia MÃ³vel Exponencial (MME) - Curto perÃ­odo 
5) SuavizaÃ§Ã£o por MÃ©dia MÃ³vel Simples (MMS) - Longo perÃ­odo

DiferenciaÃ§Ã£o -> A diferenciaÃ§Ã£o, busca transformar uma sÃ©rie nÃ£o estacionÃ¡ria em estacionÃ¡ria, por meio da diferenÃ§a de dois perÃ­odos consecutivos

#### Modelos das sÃ©ries temporais univariados:
Modelos lineares:
 - Modelos autorregressivos (AR)
 - Modelos mÃ©dias mÃ³veis (MA)
 - Modelos autorregressivos e mÃ©dias mÃ³veis (ARMA)
 - Modelos autorregressivos integrados e de mÃ©dias mÃ³veis (ARIMA)
 - Modelos de longas dependÃªncias temporais ou memÃ³ria longa (ARFIMA)
 - Modelos autorregressivos integrados e de mÃ©dias mÃ³veis com sazonalidade (SARIMA)
 
Modelos nÃ£o lineares:
 - Autorregressivo com limiar (TAR)
 - Autorregressivo com transiÃ§Ã£o suave (STAR)
 - Troca de regime markoviano (MSM)
 - Redes neurais artificiais autorregressivas (AR-ANN)

Estrutura: 
- Autorregressivo (AR): indica que a variÃ¡vel Ã© regressadaÂ em seus valores anteriores. 
- Integrado (I): indica que os valores de dados foram substituÃ­dos com a diferenÃ§a entre seus valores e os valores anteriores (diferenciaÃ§Ã£o).
- MÃ©dia mÃ³vel (MA): Indica que o erro de regressÃ£o Ã© uma combinaÃ§Ã£o linearÂ dos termos de erro dos valores passados.

CodificaÃ§Ã£o: (p, d, q) ParÃ¢metro d sÃ³ pode ser inteiro, caso estivessemos trabalhando com um Modelo ARFIMA, o parÃ¢metro d pode ser fracionado

- p = ordem da autorregressÃ£o.
- d = grau de diferenciaÃ§Ã£o.
- q = ordem da mÃ©dia mÃ³vel.

Quando adicionamos a sazonalidade, alÃ©m da codificaÃ§Ã£o Arima (p, d, q), incluimos a codificaÃ§Ã£o para a Sazonalidade (P, D, Q). EntÃ£o um modelo SARIMA Ã© definido por: (p, d, q)(P, D, Q)

Exemplos:
- Modelo ARFIMA: (1, 0.25, 1) 
- Modelo ARIMA: (2, 1, 1)
- Modelo AR: (1, 0, 0)
- Modelo MA (0, 0, 3)
- Modelo I: (0, 2, 0)
- Modelo ARMA: (4, 0, 1)
- Modelo SARIMA: (1, 1, 2)(2, 0, 1)
- 
#### FunÃ§Ã£o de AutocorrelaÃ§Ã£o (FAC) e FunÃ§Ã£o de AutocorrelaÃ§Ã£o Parcial (FACP)

#### Akaikeâ€™s Information Criterion (AIC) e o Bayesian Information Criterion (BIC)
Nos modelos mais avanÃ§ados, as funÃ§Ãµes de autocorrelaÃ§Ã£o e autocorrelaÃ§Ã£o parcial nÃ£o sÃ£o informativas para definir a ordem dos modelos, por isso usasse um critÃ©rio de informaÃ§Ã£o. Um critÃ©rio de informaÃ§Ã£o Ã© uma forma de encontrar o nÃºmero ideal de parÃ¢metros de um modelo, para entendÃª-lo, tenha em mente que, a cada regressor adicional, a soma dos resÃ­duos nÃ£o vai aumentar; frequentemente, diminuirÃ¡. A reduÃ§Ã£o se dÃ¡ Ã  custa de mais regressores. Para balancear a reduÃ§Ã£o dos erros e o aumento do nÃºmero de regressores, o critÃ©rio de informaÃ§Ã£o associa uma penalidade a esse aumento. Sendo assim, sua equaÃ§Ã£o apresenta duas partes: a primeira mede a qualidade do ajuste do modelo aos dados, enquanto a segunda parte Ã© chamada de funÃ§Ã£o de penalizaÃ§Ã£o dado que penaliza modelos com muitos parÃ¢metros, sendo assim, dado todas as combinaÃ§Ãµes de modelos procuramos aquele que apresenta menor AIC.

#### Testes EstatÃ­sticos

##### Teste de Kolmogorov-Smirnov
> Qualifica a mÃ¡xima diferenÃ§a absoluta entre a funÃ§Ã£o de distribuiÃ§Ã£o da amostra e a funÃ§Ã£o de distribuiÃ§Ã£o acumulada da distribuiÃ§Ã£o de referÃªncia (geramente distribuiÃ§Ã£o normal), ou seja, ele qualifica distÃ¢ncia entre duas amostras (comparaÃ§Ã£o entre elas).

- *H0: A amostra segue a distribuiÃ§Ã£o de referÃªncia*
- *H1: A amostra nÃ£o segue a distribuiÃ§Ã£o de referÃªncia*

##### Teste de Anderson-Darling 
> Testa se uma funÃ§Ã£o de distribuiÃ§Ã£o acumulada f(x), pode ser candidata a ser um funÃ§Ã£o de distribuiÃ§Ã£o acumulada de uma amostra aleatÃ³ria;

- *H0: A amostra tem distribuiÃ§Ã£o de f(x)*
- *H1: A amostra nÃ£o tem distribuiÃ§Ã£o f(x)*

##### Teste de Shapiro Wilk 
> O teste Shapiro Wilk segue a seguinte equaÃ§Ã£o descrita abaixo. Sendo que xi sÃ£o os valores da amostra ordenados, no qual valores menores que W sÃ£o evidÃªncias de que os dados sÃ£o normais.

![Captura de tela 2024-07-04 191812](https://github.com/HenrySchall/Time-Series/assets/96027335/c9789639-2602-44bb-a9f3-491b92b65310)

> JÃ¡ o termo b Ã© determinado pela seguinte equaÃ§Ã£o:

![Captura de tela 2024-07-04 192115](https://github.com/HenrySchall/Time-Series/assets/96027335/c2594f21-082f-4f6d-9293-66b45b0125fb)

> onde ai sÃ£o constantes geradas pelas mÃ©dias, variÃ¢ncias e covariÃ¢ncias das estatÃ­sticas de ordem de uma amostra de tamanho n de uma distribuiÃ§Ã£o normal (tabela da normal).

EstatÃ­stica de teste:
- *H0: A amostra segue uma distribuiÃ§Ã£o normal (W-obtido < W-crÃ­tico)*
- *H1: A amostra nÃ£o segue uma distribuiÃ§Ã£o normal (W-obtido > W-crÃ­tico)*

![img46](https://github.com/HenrySchall/Time-Series/assets/96027335/64ca5aa1-d601-44b1-8d21-16ec79400211)

##### Teste de Jarque-Bera
> Verifica se os erros sÃ£o um RuÃ­do Branco, ou seja, seguem uma distribuiÃ§Ã£o normal. O teste se baseia nos resÃ­duos do mÃ©todo dos mÃ­nimos quadrados. Para sua realizaÃ§Ã£o o teste necessita dos cÃ¡lculos da assimetria (skewness) e da curtose (kurtosis) da amostra, dado pela seguinte fÃ³rmula:
 
![Captura de tela 2024-07-04 193133](https://github.com/HenrySchall/Time-Series/assets/96027335/fe76cc80-fa40-46c8-8357-7e19e49339a5)

> onde n e o nÃºmero de observaÃ§Ãµes (ou graus de liberdade geral); S Ã© aassimetria da amostra; e K Ã© a curtose da amostra

![Captura de tela 2024-07-04 193243](https://github.com/HenrySchall/Time-Series/assets/96027335/b24d6ca3-6e20-44ed-a3d6-5004c3646bd6)

$\widehat{u3}$ e $\widehat{u4}$ sÃ£o as estimativas do terceiro e quarto momentos, respectivamente; $\bar{x}$ a mÃ©dia da amostra, e $ğœ^2$ Ã© a estimativa do segundo momento, a variÃ¢ncia.

- *H0: resÃ­duos sÃ£o normalmente distribuÃ­dos*
- *H1: resÃ­duos nÃ£o sÃ£o normalmente distribuÃ­dos*

#### Resumo:

|Teste|Quando usar|PrÃ³s|Contras|CenÃ¡rios nÃ£o indicados|
|---|---|---|---|---|
|Shapiro-Wilk|Pequenas amostras (sensÃ­vel a pequenas desvios da normalidade)|SensÃ­vel a pequenas desvios da normalidade (adequado para amostras pequenas|Pode ser menos potente em amostras maiores|Dados com distribuiÃ§Ã£o fortemente bimodal ou multimodal|
|Kolmogorov-Smirov|Amostras grandes (teste nÃ£o paramÃ©trico)|NÃ£o requer suposiÃ§Ãµes sobre os parÃ¢metros da distribuiÃ§Ã£o (adequado para amostras grandes)|Menos sensÃ­vel a pequenos desvios (menos potente em amostras pequenas)|SensÃ­vel a desvios nas caudas da distribuiÃ§Ã£o|
|Anderson Darling|VerificaÃ§Ã£o geral de normalidade|Sensibilidade a desvios em caudas e simetria (fornece estatÃ­stica de teste e valores crÃ­ticos)|Menos sensÃ­vel a desvios pequenos|NÃ£o Ã© recomendado para amostras muito pequenas|
|Jaque-Bera|VerificaÃ§Ã£o geral de normalidade em amostras grandes|Combina informaÃ§Ãµes sobre simetria e curtose (adequado para amostras grandes)|Menos sensÃ­vel a desvios pequenos|SensÃ­vel a desvios nas caudas da distribuiÃ§Ã£o| 
  
##### Teste de AderÃªncia
> Este teste Ã© utilizado quando deseja-se validar a hipÃ³tese que um conjunto de dados Ã© gerado por uma determinada distribuiÃ§Ã£o de probabilidade.

- *H0: segue o modelo proposto*
- *H1: nÃ£o segue o modelo proposto*
  
##### Teste de IndepedÃªncia
> Este teste Ã© utilizado quando deseja-se validar a hipÃ³tese de independÃªncia entre duas variÃ¡veis aleatÃ³rias. Se por exemplo, existe a funlÃ§ao de probabilidade conjunta das duas variÃ¡veis aleatÃ³rias, pode-se verificar se para todos os possÃ­veis valores das variÃ¡vies, o produto das probabilidades margianis Ã© igual Ã  probabilidade conjunto.

- *H0: as variÃ¡veis aleatÃ³rias sÃ£o independentes*
- *H1: as variÃ¡veis aleatÃ³rias nÃ£o sÃ£o independentes*

##### Teste de Homogeneidade
> Esse teste Ã© utilizado quando deseja-se validar a hipÃ³tese de que uma variÃ¡vel aleatÃ³ria apresenta comportamento similar, ou homogÃªneo, em relaÃ§Ã£o Ã s suas vÃ¡rias subpopulaÃ§Ãµes. Este teste apresenta a mesma mecÃ¢nica do Teste de IndependÃªncia, mas uma distinÃ§Ã£o importante se refere Ã  forma como as amostras sÃ£o coletadas. No Teste de homogeneidade fixa-se o tamanho da amostra em cada uma das subpopulaÃ§Ãµes e, entÃ£o, seleciona-se uma amostra de cada uma delas.

- *H0: As subpopulaÃ§Ãµes das variÃ¡veis aleatÃ³rias sÃ£o homogÃªneas*
- *H1: As subpopulaÃ§Ãµes das variÃ¡veis aleatÃ³rias nÃ£o sÃ£o homogÃªneas*
  

#### Coeficientes de CorrelaÃ§Ã£o
> Os coeficientes de correlaÃ§Ã£o verificam a existÃªncia e o grau de associaÃ§Ã£o entre dois conjuntos de dados.

##### Coeficiente Pearson 
> Estabelecer o nÃ­vel de relaÃ§Ã£o linear entre duas variÃ¡veis. Em outras palavras, mede em grau e o sentido (crescente/decrescente) da associaÃ§Ã£o linear entre duas variÃ¡veis. Ele sempre estarÃ¡ entre âˆ’1,00 e +1,00, tendo o sinal a funÃ§Ã£o de indicar a direÃ§Ã£o do movimento, ou seja, positivo (relaÃ§Ã£o direta) e negativa (relaÃ§Ã£o inversa) e o valor do coeficiente, a funÃ§Ã£o de indicar a forÃ§a da correlaÃ§Ã£o, onde nos intervalos:
> - (+0,90; +1,00) ou (âˆ’1,00; âˆ’0,90) = correlaÃ§Ã£o muito forte
> - (+0,60; +0,90) ou (âˆ’0,90; âˆ’0,60) = correlaÃ§Ã£o forte
> - (+0,30; +0,60) ou (âˆ’0,60; âˆ’0,30) = correlaÃ§Ã£o moderada
> - (0,00; +0,30) ou (âˆ’0,30; 0,00) = correlaÃ§Ã£o fraca
>
> Graficamente:

![3](https://github.com/HenrySchall/Time-Series/assets/96027335/5391579e-90f0-4ed2-92a0-b95c6068591f)

> Sua equaÃ§Ã£o Ã© definida pela seguinte fÃ³rmula:

![1](https://github.com/HenrySchall/Time-Series/assets/96027335/8f4dd7f6-e82b-4bf0-a06d-58400ede1060)

> Lembrando que o coeficiente de correlaÃ§Ã£o populacional Ã© dado por:

![2](https://github.com/HenrySchall/Time-Series/assets/96027335/6e39a2b7-4bfd-4d30-987e-bca3e8c5c8d8)

> Exemplo: A tabela abaixo apresenta 15 observaÃ§Ãµes, com o tempo de entrega (em minutos) e a distÃ¢ncia de entrega de um TelePizza.

|Tempo|DistÃ¢ncia|
|---|---|
|40|688|
|21|215|
|14|255|
|20|462|
|24|448|
|29|776|
|15|200|
|19|132|
|10|36|
|35|770|
|18|140|
|52|810|
|19|450|
|20|635|
|11|150|

> Calculando os valores obtemos o seguinte resultado:

![4](https://github.com/HenrySchall/Time-Series/assets/96027335/a224016f-5d0a-4b84-ac79-6b8f5dae6ae1)

> Conclui-se que existe uma relaÃ§Ã£o linear forte e positiva entre as variÃ¡veis. Todavia o coeficiente de correlaÃ§Ã£o de Pearson Ã© apenas uma estimativa do coeficiente de correlaÃ§Ã£o populacional, pois Ã© calculado com base em uma amostra aleatÃ³ria de ğ‘› pares de dados. Sendo assim a amostra observada pode apresentar correlaÃ§Ã£o, mas a populaÃ§Ã£o nÃ£o, neste caso, tem-se um problema de inferÃªncia, pois o fato de râ‰ 0 nÃ£o Ã© garantia de ğœŒâ‰ 0. Para resolver esse problema, utiliza-se da estatÃ­stica de teste T-student, definido pela equaÃ§Ã£o abaixo, para verificar se realmente existe correlaÃ§Ã£o linear entre as variÃ¡veis:

![5](https://github.com/HenrySchall/Time-Series/assets/96027335/84310f44-b3d8-477d-9e71-1ec81dcbb21a)

> Onde ğ‘¡ segue uma distribuiÃ§Ã£o ğ‘¡âˆ’ğ‘†ğ‘¡ğ‘¢ğ‘‘ğ‘’ğ‘›ğ‘¡ com ğ‘›âˆ’2 graus de liberdade e regido pelas seguintes hipÃ³teses:

- *H0: A correlaÃ§Ã£o entre as variÃ¡veis Ã© zero (ğœŒ = 0)*
- *H1: A correlaÃ§Ã£o entre as variÃ¡veis nÃ£o Ã© zero (ğœŒ â‰  0)*

![6](https://github.com/HenrySchall/Time-Series/assets/96027335/ccc5a428-cea3-4a8f-a773-c6b454b87f28)

> A partir da estatÃ­stica ğ‘¡ com 13 graus de liberdade, os pontos crÃ­ticos sÃ£o Â±2,1604. Portanto, rejeita-se ğ»ğ‘œ ao nÃ­vel de significÃ¢ncia de 5%. Sendo assim a correlaÃ§Ã£o entre o tempo de entrega e a distÃ¢ncia percorrida Ã© diferente de zero, entÃ£o, existe uma relaÃ§Ã£o linear e positiva entre as variÃ¡veis da ordem de ğ‘Ÿ = 0,8216.

##### Coeficiente Spearman
> O coeficiente de correlaÃ§Ã£o de Spearman, ou rho de Spearman, Ã© uma medida nÃ£o paramÃ©trica da correlaÃ§Ã£o (associaÃ§Ã£o) entre duas variÃ¡veis ordinais. Ao contrÃ¡rio do coeficiente de correlaÃ§Ã£o de Pearson, que mede a forÃ§a e a direÃ§Ã£o da relaÃ§Ã£o linear entre duas variÃ¡veis, o coeficiente de Spearman avalia a intensidade (o quÃ£o bem) Ã© a relaÃ§Ã£o entre duas variÃ¡veis. O coeficiente de correlaÃ§Ã£o de Spearman (ğœŒ) Ã© calculado utilizando a seguinte fÃ³rmula:

![20](https://github.com/HenrySchall/Time-Series/assets/96027335/65f56f8e-31b3-4d4f-a4d0-b7e692b2fd44)

#### InterpretaÃ§Ã£o:
- Ï=1 indica uma perfeita correlaÃ§Ã£o positiva.
- Ï=âˆ’1 indica uma perfeita correlaÃ§Ã£o negativa.
- Ï=0 indica ausÃªncia de correlaÃ§Ã£o.

> Exemplo: Dados os valores da tabela abaixo:

![9](https://github.com/HenrySchall/Time-Series/assets/96027335/68db17b8-2b31-41c9-b1d1-cf241b30ee55)

> Calculando os valores obtemos o seguinte resultado:

![12](https://github.com/HenrySchall/Time-Series/assets/96027335/1c332788-5570-48e1-8d17-2b3b36c61aff)
 
> Utilizando-se da mesma equaÃ§Ã£o estatÃ­stica do teste T-student. Teremos as seguintes hipÃ³teses:

- *H0: A correlaÃ§Ã£o entre as variÃ¡veis Ã© zero*
- *H1: A correlaÃ§Ã£o entre as variÃ¡veis nÃ£o Ã© zero*

> A partir da estatÃ­stica ğ‘¡âˆ’ğ‘†ğ‘¡ğ‘¢ğ‘‘ğ‘’ğ‘›ğ‘¡ com 11 graus de liberdade, os pontos crÃ­ticos sÃ£o Â±2,2010. Portanto, rejeita-se ğ»ğ‘œ ao nÃ­vel de significÃ¢ncia de 5%. Sendo assim a correlaÃ§Ã£o entre as variÃ¡veis ğ‘‹ e ğ‘Œ Ã© diferente 
de zero, entÃ£o, existe uma relaÃ§Ã£o nÃ£o-linear e negativa de ordem ğ‘Ÿ= âˆ’0,9698. 

##### Coeficiente Kendall 
> O coeficiente de correlaÃ§Ã£o de Kendall Ã© uma medida estatÃ­stica utilizada para avaliar a associaÃ§Ã£o entre duas variÃ¡veis ordinais, exatamente igual ao coeficiente de correlaÃ§Ã£o de Spearman, a difenreÃ§a Ã© que ele mede a correlaÃ§Ã£o de concordÃ¢ncia, enquanto Spearman, mede a correlaÃ§Ã£o de postos. Sendo particularmente Ãºtil quando as variÃ¡veis em questÃ£o nÃ£o assumem necessariamente distribuiÃ§Ãµes normais. O coeficiente de correlaÃ§Ã£o de Kendall (Ï„) Ã© definido pela seguinte fÃ³rmula:

![124](https://github.com/HenrySchall/Time-Series/assets/96027335/ac68ba9a-1c0d-4cab-a892-7d9ab87cc400)

> No qual, ğ‘› Ã© o nÃºmero de elementos aos quais atribui-se postos, ğ‘† Ã© a soma da variÃ¡vel ğ‘Œ Ã  direita que sÃ£o superiores menos o nÃºmero de postos Ã  direita que sÃ£o inferiores.

#### InterpretaÃ§Ã£o:
- Ï„=1 indica uma perfeita concordÃ¢ncia.
- Ï„=âˆ’1 indica uma perfeita discordÃ¢ncia.
- Ï„=0 indica ausÃªncia de associaÃ§Ã£o entre as variÃ¡veis.

> Para o cÃ¡lculo do coeficiente de correlaÃ§Ã£o por postos de Kendall ordena-se inicialmente uma das variÃ¡veis em ordem crescente de postos e o S correspondente a cada elemento serÃ¡ obtido fazendo o nÃºmero de elementos 
cujo posto Ã© superior ao que se estÃ¡ calculando menos o nÃºmero de elementos cujo posto Ã© inferior ao mesmo. Para verificar a significÃ¢ncia do valor observado do coeficiente ğœ de Kendall, para ğ‘›â‰¤10 deve-se consultar a tabela abaixo.

![125](https://github.com/HenrySchall/Time-Series/assets/96027335/9a2f16b5-e667-4d3c-a8dd-01d3ed678409)

> Para ğ‘›>10, pode utilizar a estatÃ­stica de teste:

![128](https://github.com/HenrySchall/Time-Series/assets/96027335/275d3eda-ef7e-453a-86d1-a4918ef935ff)

> Exemplo: Dados os valores da tabela abaixo:

![14](https://github.com/HenrySchall/Time-Series/assets/96027335/a886a3c7-1e07-429f-844f-782e88c480c7)

> Calculando os valores obtemos o seguinte resultado:

![17](https://github.com/HenrySchall/Time-Series/assets/96027335/aae35d85-d483-40ad-a41e-6261c2b36b97)

> Tendo as seguintes hipÃ³teses:

- *H0: A correlaÃ§Ã£o entre as variÃ¡veis Ã© zero (ğœ=0)*
- *H1: A correlaÃ§Ã£o entre as variÃ¡veis nÃ£o Ã© zero (ğœâ‰ 0)*

>  A partir da estatÃ­stica ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ ğ‘ğ‘ğ‘‘ğ‘ŸÃ£ğ‘œ, os pontos crÃ­ticos sÃ£o Â±1,96. Portanto, rejeita-se ğ»ğ‘œ ao nÃ­vel de significÃ¢ncia de 5%. Sendo assim a correlaÃ§Ã£o entre as variÃ¡veis ğ‘‹ e ğ‘Œ Ã© diferente de zero, entÃ£o, existe uma 
relaÃ§Ã£o nÃ£o-linear e negativa de ordem ğœ=âˆ’0,7692.

*ObservaÃ§Ã£o: Pode-se fazer uma comparaÃ§Ã£o entre coeficiente de correlaÃ§Ã£o de Spearman e o coeficiente de correlaÃ§Ã£o por postos de Kendall. Os valores numÃ©ricos nÃ£o sÃ£o iguais, quando calculados para os mesmos pares de postos, e nÃ£o sÃ£o comparÃ¡veis numericamente. Contudo, pelo fato de utilizarem a mesma quantidade de informaÃ§Ã£o contida nos dados, ambos tÃªm o mesmo poder de detectar a existÃªncia de associaÃ§Ã£o na populaÃ§Ã£o, e rejeitarÃ£o a hipÃ³tese nula para um mesmo nÃ­vel de significÃ¢ncia.*








